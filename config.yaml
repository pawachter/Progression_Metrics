models:
  CNN:
    input_shape: [32, 32, 3]
    num_classes: 10
    padding: 'same'  # Use same padding for convolutions
    conv_layers:
      - filters: 32
        kernel_size: [3, 3]
        activation: 'relu'
      - filters: 64
        kernel_size: [3, 3]
        activation: 'relu'
    batch_norm: true
    pooling:
      enabled: true
      type: 'max'  # Options: 'max' or 'average'
      pool_size: [2, 2]
    dropout_rate: 0.25
    dense_layers:
      - units: 128
        activation: 'relu'
    final_activation: 'softmax'

  VisionTransformer:
    input_shape: [32, 32, 3]
    num_classes: 10
    patch_size: [4, 4]
    num_patches: 64
    projection_dim: 64
    transformer_layers:
      - num_heads: 4
        mlp_units: [128, 64]
        dropout_rate: 0.1
    dense_units: 128
    dropout_rate: 0.1
    final_activation: 'softmax'

  MLP:
    input_shape: [32, 32, 3]
    num_classes: 10
    flatten: true
    dense_layers:
      - units: 128
        activation: 'relu'
      - units: 64
        activation: 'relu'
    dropout_rate: 0.25
    final_activation: 'softmax'
    
training:
  epochs: 50
  batch_size: 64
  optimizer: 'adam'
  learning_rate: 0.001
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  early_stopping_patience: 3

domain_adaptation:
  enabled: false  # Set to true to enable domain adaptation tracking
  subset_size: 500  # Number of samples to hold out from each domain for DA metrics
  n_steps: 100  # Compute DA metrics every N training steps
  
  # Target Domain Gap tracking
  target_gap:
    k_window: 10  # Window size for loss history
    ema_alpha: 0.9  # Exponential moving average smoothing factor
  
  # Entropy Gap tracking
  entropy_gap:
    batch_size: 64  # Batch size for entropy computation
  
  # Representation Mismatch tracking
  representation_mismatch:
    batch_size: 64  # Batch size for representation extraction
    mmd_kernel: 'rbf'  # Kernel type for MMD: 'rbf' or 'linear'
    mmd_gamma: 1.0  # Bandwidth parameter for RBF kernel

callbacks: {}
